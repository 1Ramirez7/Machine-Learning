---
title: "OLS test"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 5
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false 
---


```{python}

# test 4 ----------- same as test 3 but this mdoel automatically test all different variables as dependent variables---- 


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the data
file_path = "https://raw.githubusercontent.com/1Ramirez7/Machine-Learning/refs/heads/main/bank_project/camp.csv"
data = pd.read_csv(file_path)

# Iterate through each column in the DataFrame
for column in data.columns:
    print(f"Evaluating model with dependent variable: {column}")

    # Set the current column as the dependent variable
    y = data[column]
    X = data.drop(column, axis=1)

    # Handling missing values
    X_filled = X.fillna(X.mean())
    y_filled = y.fillna(y.mean())

    # Split the dataset
    X_train, X_test, y_train, y_test = train_test_split(X_filled, y_filled, test_size=0.2, random_state=42)

    # Define and train the model
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Evaluate the model
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"Mean Squared Error: {mse}")
    print(f"R^2 Score: {r2}\n")

    # Optionally, you can also print the coefficients for the model
    # for i, col in enumerate(X_train.columns):
    #     print(f"{col}: {model.coef_[i]}")



```




```{python}
# test 8 ----- same as test 7 & 6 but w/out the sklearn import
# ---------- option to change between reports- - - -- - - - - - -

import pandas as pd
import statsmodels.api as sm

# Load the data
file_path = "https://raw.githubusercontent.com/1Ramirez7/Machine-Learning/refs/heads/main/bank_project/camp.csv"
data = pd.read_csv(file_path)

# Columns to exclude
#columns_to_exclude = ['Quarter'] # 'column_name_2', 'column_name_3'
#data = data.drop(columns=columns_to_exclude)

# Iterate
for column in data.columns:
    print(f"Evaluating OLS model with dependent variable: {column}")
    y = data[column]
    X = data.drop(column, axis=1)

    # Handling missing values
    X_filled = X.fillna(X.mean())
    y_filled = y.fillna(y.mean())

    # Add a constant to the independent variables
    X_filled = sm.add_constant(X_filled)

    # Define and fit the OLS model
    model = sm.OLS(y_filled, X_filled)
    results = model.fit()


    # Print 
    
    print(f"R^2 Score: {results.rsquared}")

    print("Additional Details:")
    print(f"R^2 Score: {results.rsquared}")
    print(f"Adjusted R^2 Score: {results.rsquared_adj}")
    print(f"Mean Squared Error: {results.mse_model}")

    print(results.summary())




```







```{python}

# This code tries to compare with the y variable, but need to filter more because its to slow and some zero div errors

import pandas as pd
import plotly.graph_objects as go
import numpy as np
from statsmodels.nonparametric.smoothers_lowess import lowess
from plotly.subplots import make_subplots


# Load data
data = pd.read_csv("https://raw.githubusercontent.com/1Ramirez7/Machine-Learning/refs/heads/main/bank_project/camp.csv")

# Function to create scatter plot traces
def create_scatter_plot_traces(x_column_name, y_column_name, data):
    lowess_result = lowess(data[y_column_name], data[x_column_name])
    smoothed_y = lowess_result[:, 1]
    x_values = lowess_result[:, 0]

    ci = 0.1 * smoothed_y

    traces = [
        go.Scatter(
            x=data[x_column_name], 
            y=data[y_column_name], 
            mode='markers', 
            name='Data',
            hovertemplate=(
                f"<b>{x_column_name}:</b> %{{x}}<br>"
                f"<b>{y_column_name}:</b> %{{y}}<br>"
            ),
            showlegend=False
        ),
        go.Scatter(
            x=x_values, 
            y=smoothed_y, 
            mode='lines', 
            name='Lowess Fit',
            showlegend=False
        ),
        go.Scatter(
            x=np.concatenate([x_values, x_values[::-1]]), 
            y=np.concatenate([smoothed_y - ci, (smoothed_y + ci)[::-1]]), 
            fill='toself', 
            fillcolor='rgba(0,100,80,0.2)', 
            line=dict(color='rgba(255,255,255,0)'), 
            hoverinfo="skip", 
            name='CI',
            showlegend=False
        )
    ]

    return traces

# Function to create a subplot figure
def create_subplot_figure(data):
    y_column_name = 'y1'
    # Exclude your Y variable and "Quarter" from X columns
    x_columns = [
        col for col in data.columns 
        if col not in [y_column_name]
    ]

    num_plots = len(x_columns)
    rows = (num_plots - 1) // 4 + 1
    cols = min(4, num_plots)

    fig = make_subplots(rows=rows, cols=cols, subplot_titles=x_columns)

    for i, x_column in enumerate(x_columns, start=1):
        for trace in create_scatter_plot_traces(x_column, y_column_name, data):
            fig.add_trace(trace, row=(i-1)//4 + 1, col=(i-1)%4 + 1)

    fig.update_layout(
        height=200 * rows, 
        width=1200, 
        title_text="Scatter Plots with Lowess Fit"
    )
    
    fig.show()


# Create and display the figure
create_subplot_figure(data)




```



The below model runs an ols regression model but it is not the best fit for a binary classification. IT is better to run a logistic regression or a classification tree. However this model does great in understanding statistical significance of the variables. 


```{python}

# -----------------First Regression------------------
import pandas as pd
from sklearn.linear_model import LinearRegression
from statsmodels.regression.linear_model import OLS
import statsmodels.api as sm
file_path = "https://raw.githubusercontent.com/1Ramirez7/Machine-Learning/refs/heads/main/bank_project/camp.csv"
df = pd.read_csv(file_path)
X = df[['age', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 
         'euribor3m', 'nr.employed', 'marital1', 'education1', 'default1', 'housing1', 
         'loan1', 'contact1', 'month1', 'day_of_week1', 'poutcome1', 'job1']]
y = df['y1']
X_with_constant = sm.add_constant(X)
model = OLS(y, X_with_constant).fit()
print(model.summary())


```




This next code is doing a logistic regression. I'm not sure of the best variables to use yet. 

```{python}

X = df.drop('y1', axis=1)
y = df['y1']

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report


# Example: separate features (X) and target (y)
X = df.drop('y1', axis=1)
y = df['y1']

# Split into training (80%) and test (20%) sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42
)


# 1. Instantiate logistic regression
logreg = LogisticRegression(solver='liblinear', random_state=42)

# 2. Train (fit) the model
logreg.fit(X_train, y_train)

# 3. Predict on the test set
y_pred_logreg = logreg.predict(X_test)

# 4. Evaluate the model
accuracy_logreg = accuracy_score(y_test, y_pred_logreg)
auc_logreg = roc_auc_score(y_test, y_pred_logreg)
report_logreg = classification_report(y_test, y_pred_logreg)

print("=== Logistic Regression Results ===")
print(f"Accuracy: {accuracy_logreg:.4f}")
print(f"ROC AUC:  {auc_logreg:.4f}")
print("Classification Report:")
print(report_logreg)



```


The code below is using a Decision Tree Classifier, but not ready to use since idk what is mostly doing

```{python}

# 1. Instantiate decision tree
dtree = DecisionTreeClassifier(max_depth=5, random_state=42)

# 2. Fit the tree
dtree.fit(X_train, y_train)

# 3. Predict on the test set
y_pred_tree = dtree.predict(X_test)

# 4. Evaluate the model
accuracy_tree = accuracy_score(y_test, y_pred_tree)
auc_tree = roc_auc_score(y_test, y_pred_tree)
report_tree = classification_report(y_test, y_pred_tree)

print("=== Decision Tree Results ===")
print(f"Accuracy: {accuracy_tree:.4f}")
print(f"ROC AUC:  {auc_tree:.4f}")
print("Classification Report:")
print(report_tree)




```



The following does a scatterplot to vizualize

```{python}
import pandas as pd
import matplotlib.pyplot as plt

file_path = "https://raw.githubusercontent.com/1Ramirez7/Machine-Learning/refs/heads/main/bank_project/camp.csv"

data = pd.read_csv(file_path)

# Plotting
plt.figure(figsize=(10, 6))
plt.scatter(data['y1'], data['job1'], alpha=0.6)
plt.title("Scatter Plot of y1 vs job1")
plt.xlabel("y1")
plt.ylabel("job1")
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()


```

the below model does a scatter plot with y1 in the x axis for all variables.

```{python}

import pandas as pd
import matplotlib.pyplot as plt

# Load the data
file_path = "https://raw.githubusercontent.com/1Ramirez7/Machine-Learning/refs/heads/main/bank_project/camp.csv"
data = pd.read_csv(file_path)

# Identify variables for plotting
variables = data.columns.drop('y1')  # Exclude y1 since it's always on the x-axis

# Create plots
for variable in variables:
    plt.figure(figsize=(8, 5))
    plt.scatter(data['y1'], data[variable], alpha=0.6)
    plt.title(f"Scatter Plot of y1 vs {variable}")
    plt.xlabel("y1")
    plt.ylabel(variable)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.show()



```


default1 value of 3 only applies to y1 value of 1

nr.employed value of 5176.3 only applies to y1 value of 1

euribor3m values from 3 to 4 only applies to y1 value of 1

campaign values above 17 only apply to y1 value of 1